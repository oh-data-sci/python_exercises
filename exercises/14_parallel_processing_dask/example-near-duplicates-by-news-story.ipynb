{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real example: finding near-duplicate headlines in a collection of news stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a real case I've run into in my work at Brandwatch where parallelising the work on a cluster would have speeded up my progress. (It might be a slightly odd example but at least it's a real one!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "from dask.distributed import Client\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample dataset of news headlines from 9 stories, with 1000 headlines from each story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sample_news_headlines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sensational: Hungarian Junior Handball Team triumphs at World Championship! | Daily News Hungary</td>\n",
       "      <td>france-football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>World Cup afterglow gives France a sorely needed boost</td>\n",
       "      <td>france-football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>World Cup final 2018: The sad story of Ivan Perisic, today's decisive player but not always in the way he might want</td>\n",
       "      <td>france-football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LIVE| FIFA World Cup 2018 final: History awaits Croatia in summit clash vs France</td>\n",
       "      <td>france-football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VAR used for first time in World Cup final</td>\n",
       "      <td>france-football</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  title  \\\n",
       "0  Sensational: Hungarian Junior Handball Team triumphs at World Championship! | Daily News Hungary                       \n",
       "1  World Cup afterglow gives France a sorely needed boost                                                                 \n",
       "2  World Cup final 2018: The sad story of Ivan Perisic, today's decisive player but not always in the way he might want   \n",
       "3  LIVE| FIFA World Cup 2018 final: History awaits Croatia in summit clash vs France                                      \n",
       "4  VAR used for first time in World Cup final                                                                             \n",
       "\n",
       "             story  \n",
       "0  france-football  \n",
       "1  france-football  \n",
       "2  france-football  \n",
       "3  france-football  \n",
       "4  france-football  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>stephen-hawking</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brexit-resignations</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nhs-cyber-attack</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>may-deal</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>france-football</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macron</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nhs-70-years</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brexit-trump</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gatwick-drone</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Frequency\n",
       "stephen-hawking      1000     \n",
       "brexit-resignations  1000     \n",
       "nhs-cyber-attack     1000     \n",
       "may-deal             1000     \n",
       "france-football      1000     \n",
       "macron               1000     \n",
       "nhs-70-years         1000     \n",
       "brexit-trump         1000     \n",
       "gatwick-drone        1000     "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['story'].value_counts().to_frame(name='Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some code, self-plagiarised from a previous workshop, for identifying and excluding near-duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is one way to define when two strings are near-duplicates, using their longest common substring (LCS).\n",
    "\n",
    "def is_near_duplicate(str1, str2):\n",
    "\n",
    "    lcs_fraction_threshold = 0.50\n",
    "    \n",
    "    min_length_threshold = 40   # For very short strings things get a bit weird, so we have a threshold on that too\n",
    "\n",
    "    str1 = str1.casefold()\n",
    "    str2 = str2.casefold()\n",
    "    \n",
    "    len1 = len(str1)\n",
    "    len2 = len(str2)\n",
    "    \n",
    "    # Be careful with SequenceMatcher; if you don't pass autojunk=False it will use its \"autojunk heuristic\"\n",
    "    # (which you can read about online) and you won't get all the matches you expect.\n",
    "    \n",
    "    match = SequenceMatcher(None, str1, str2, autojunk=False).find_longest_match(0, len1, 0, len2)\n",
    "\n",
    "    if (    (match.size > lcs_fraction_threshold * len1 and len1 >= min_length_threshold) \n",
    "             or  (match.size > lcs_fraction_threshold * len2 and len2 >= min_length_threshold)):\n",
    "        return True, str1[match.a: (match.a + match.size)]\n",
    "    else:\n",
    "        return False, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function excludes near-duplicate mentions from a dataframe, as defined above by is_near_duplicate\n",
    "\n",
    "def exclude_near_dupes(df, text_col_name):\n",
    "\n",
    "    df = df.copy()\n",
    "    num_rows = df.shape[0]\n",
    "\n",
    "    # Create new columns with indices to match the dataframe.\n",
    "    accepted       = df[text_col_name].apply(lambda x: False)\n",
    "    near_dupe_pos  = df[text_col_name].apply(lambda x: -1)\n",
    "    near_dupe_text = df[text_col_name].apply(lambda x: None)\n",
    "    common_text    = df[text_col_name].apply(lambda x: None)\n",
    "\n",
    "    for i in range(num_rows):\n",
    "\n",
    "        acc = True\n",
    "\n",
    "        for j in range(i):\n",
    "\n",
    "            b, s = is_near_duplicate(df[text_col_name].iloc[i], df[text_col_name].iloc[j])\n",
    "\n",
    "            if b and accepted.iloc[j]:\n",
    "                acc = False\n",
    "                pos = j\n",
    "                dupe_txt = df[text_col_name].iloc[j]\n",
    "                common_txt = s\n",
    "\n",
    "                break\n",
    "\n",
    "        if acc:\n",
    "            accepted.iloc[i] = True\n",
    "\n",
    "        else:\n",
    "            near_dupe_pos.iloc[i]  = pos\n",
    "            near_dupe_text.iloc[i] = dupe_txt\n",
    "            common_text.iloc[i]    = common_txt\n",
    "\n",
    "\n",
    "    df['NearDupePos']  = near_dupe_pos\n",
    "    df['NearDupeText'] = near_dupe_text\n",
    "    df['CommonText']   = common_text\n",
    "\n",
    "    accepted_df = df.loc[accepted].copy().drop(columns=['NearDupePos', 'NearDupeText', 'CommonText'])\n",
    "    rejected_df = df.loc[~ accepted].copy()\n",
    "\n",
    "    return accepted_df, rejected_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string matching operations used to detect near-duplicates take quite a bit of CPU, and the cost grows quadratically with the number of rows (because we're testing each pair of rows).\n",
    "\n",
    "Here we run it just on the headlines from the France football story:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.8 s, sys: 62.6 ms, total: 40.9 s\n",
      "Wall time: 40.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "accepted_df, rejected_df = exclude_near_dupes(df=df.loc[df['story'] == \"france-football\"], text_col_name='title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus if we work through each news story in turn, in a sequential way, we will end up waiting a little while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_locally():\n",
    "    \n",
    "    story_dfs = list(map(lambda story: df.loc[df['story'] == story], df['story'].unique()))\n",
    "    results = list(map(lambda story_df: exclude_near_dupes(story_df, text_col_name='title'), story_dfs))\n",
    "\n",
    "    accepted_dfs = list(map(lambda x: x[0], results))\n",
    "    rejected_dfs = list(map(lambda x: x[1], results))\n",
    "    \n",
    "    return pd.concat(accepted_dfs, ignore_index=True), pd.concat(rejected_dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 26s, sys: 5.84 s, total: 6min 32s\n",
      "Wall time: 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "local_accepted_df, local_rejected_df = run_locally()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's process the new stories in parallel, using our cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client('127.0.0.1:8786')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:8786\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>12</li>\n",
       "  <li><b>Cores: </b>12</li>\n",
       "  <li><b>Memory: </b>48.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://172.16.0.27:8786' processes=12 cores=12>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way I've written the code above, the only changes needed to make it run distributed instead are:\n",
    "\n",
    "1. **We used `client.map` instead of a regular `map`**\n",
    "2. **We needed an extra called to `client.gather` to gather in the results**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_distributed():\n",
    "    \n",
    "    story_dfs = list(map(lambda story: df.loc[df['story'] == story], df['story'].unique()))\n",
    "\n",
    "    results_futures = client.map(lambda story_df: exclude_near_dupes(story_df, text_col_name='title'), story_dfs)\n",
    "    results = client.gather(results_futures)\n",
    "\n",
    "    accepted_dfs = list(map(lambda x: x[0], results))\n",
    "    rejected_dfs = list(map(lambda x: x[1], results))\n",
    "    \n",
    "    return pd.concat(accepted_dfs, ignore_index=True), pd.concat(rejected_dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 148 ms, sys: 20.1 ms, total: 169 ms\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "distributed_accepted_df, distributed_rejected_df = run_distributed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_accepted_df.equals(local_accepted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_rejected_df.equals(local_rejected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem was well-suited to a solution with `client.map` because:\n",
    "\n",
    " - The work was **easy to break up** into chunks of equal sizes (in fact, it was basically already broken up that way, because of the different news stories)\n",
    " - For each news story **the data was fairly small** - meaning that serialising/pickling it and sending it over the network was quick - **but the CPU required was high**\n",
    "\n",
    "Another problem like this would be doing a grid search of hyperparameters for a model fitting and evaluation task, where the training and testing data is moderately sized.\n",
    "\n",
    "For some types of problems, the work does not break down into chunks as neatly as this, in which case it becomes tedious for us as data scientists to have to program the division of work into subtasks. At this point we can start to use dask's **higher level APIs such as distributed dataframes**, which are similar to Spark's but **aim to behave as much like pandas dataframes as possible**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
