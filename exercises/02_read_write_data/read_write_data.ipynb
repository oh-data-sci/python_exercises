{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reading data from files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this exercise we will read data from files on disk. \n",
    "\n",
    "there are various data formats that we may expect to come across. the methods for reading each of these differs slightly. however there are some basic commonalities that i hope to make clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this week's exercise\n",
    "\n",
    "the `datafiles` folder contains an excel file `example_filters.xlsx` that peter sent me. \n",
    "\n",
    "it is an example of the output describing queries from the brandwatch app that peter needs to deals with, and he wants to build a function which can read such files and convert into a dictionary or a list of dictionaries so that he can pass the values into the `get_chart()` function.\n",
    "\n",
    "the file does *not* contain a flat table, but rather contains two columns, one containing keys, and the other containing values. each query generates a set of rows like that, but not all queries have all the available variables populated so the number of rows varies.\n",
    "\n",
    "here is an example of the `get_chart` function call:\n",
    "```\n",
    "MyChart2 = queries.get_chart(\n",
    "    name=\"[CBG] Scotch-Brite: Spring Cleaning\", \n",
    "    startDate=\"2017-09-01\", \n",
    "    endDate=\"2017-12-01\",\n",
    "    y_axis=\"volume\", \n",
    "    x_axis=\"days\", \n",
    "    breakdown_by=\"queries\",\n",
    "    )\n",
    "```\n",
    "and the data comes in a spreadsheet format akin to this ![screenshot of example_filters.xlsx](images/excel_sheet_screenshot.png). an acceptable solution first converts this file to csv (using, e.g. `csvkit`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the `open()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we start with a small and simple csv file listing the rodent inspections of new york city `NY_rodent_inspections_sample_small.csv`, because why not. \n",
    "\n",
    "we need to start by opening a file connection to the file with the built-in `open()` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_io.TextIOWrapper'>\n",
      "the file name is datafiles/NY_rodent_inspections_sample_small.csv\n",
      "the connection mode is r\n",
      "the file encoding is UTF-8\n",
      "is the file connection closed? False\n"
     ]
    }
   ],
   "source": [
    "filename_csv = 'datafiles/NY_rodent_inspections_sample_small.csv'\n",
    "infile = open(filename_csv) \n",
    "# what is this object like?\n",
    "print(type(infile)) # the file handle is an io wrapper and contains metadata about the file connection\n",
    "print('the file name is', infile.name)        \n",
    "print('the connection mode is', infile.mode)   # default mode\n",
    "print('the file encoding is', infile.encoding) # default encoding\n",
    "#print('the file newline', infile.newlines)     # default newline setting\n",
    "print('is the file connection closed?', infile.closed) # nope! we just opened it!\n",
    "#print('does the file connection use line_buffering', infile.line_buffering) # nope, by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the modes of `open()`\n",
    "- 'r' means open for reading (default)\n",
    "- 'w' means open for writing, truncating (deleting) the file first\n",
    "- 'x' means open for exclusive creation, failing if the file already exists\n",
    "- 'a' means open for writing, appending to the end of the file in case it exists\n",
    "- 'b' means binary mode\n",
    "- 't' means text mode (default)\n",
    "- '+' means open a disk file for updating (reading and writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSPECTION_TYPE,JOB_TICKET_OR_WORK_ORDER_ID,JOB_ID,JOB_PROGRESS,BBL,BORO_CODE,BLOCK,LOT,HOUSE_NUMBER,STREET_NAME,ZIP_CODE,X_COORD,Y_COORD,LATITUDE,LONGITUDE,BOROUGH,INSPECTION_DATE,RESULT,APPROVED_DATE,LOCATION\n",
      "BAIT,1,PO12965,3,1011470035,1,01147,0035,104,WEST 76 STREET,10023,990505,223527,40.7802039792471,-73.9774144709456,Manhattan,10/14/2009 12:00:27 PM,Bait applied,10/14/2009 03:01:46 PM,\"(40.7802039792471, -73.9774144709456)\"\n",
      "BAIT,2,PO12966,3,1011470034,1,01147,0034,102,WEST 76 STREET,10023,990516,223521,40.7801875030438,-73.977374757787,Manhattan,10/14/2009 12:51:21 PM,Bait applied,10/14/2009 03:02:30 PM,\"(40.7801875030438, -73.977374757787)\"\n",
      "BAIT,30,PO16966,3,2043370027,2,04337,0027,620,THWAITES PLACE,10467,1020110,252216,40.8588765781972,-73.8703636422023,Bronx,11/09/2009 12:59:55 PM,Bait applied,11/10/2009 02:54:52 PM,\"(40.8588765781972, -73.8703636422023)\"\n",
      "BAIT,31,PO13665,3,2037670077,2,03767,0077,1227,WHITEPLAINS ROAD,10472,1022441,242180,40.8313209626148,-73.861994089899,Bronx,11/09/2009 11:10:16 AM,Bait applied,11/10/2009 02:56:42 PM,\"(40.8313209626148, -73.861994089899)\"\n",
      "BAIT,38,PO11291,3,1011690057,1,01169,0057,2199,BROADWAY,10024,989641,224567,40.7830590725833,-73.9805333640688,Manhattan,11/10/2009 08:40:42 AM,Bait applied,11/17/2009 11:39:11 AM,\"(40.7830590725833, -73.9805333640688)\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now we can read the whole file (only stops at eof)\n",
    "print(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSPECTION_TYPE,JOB_TICKET_OR_WORK_ORDER_ID,JOB_ID,JOB_PROGRESS,BBL,BORO_CODE,BLOCK,LOT,HOUSE_NUMBER,STREET_NAME,ZIP_CODE,X_COORD,Y_COORD,LATITUDE,LONGITUDE,BOROUGH,INSPECTION_DATE,RESULT,APPROVED_DAT\n"
     ]
    }
   ],
   "source": [
    "# we can rewind read head to start of file\n",
    "infile.seek(0)\n",
    "# if needed, we could also just read a fixed number of characters ahead\n",
    "print(infile.read(200)) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E,LOCATION\n",
      "BAIT,1,PO12965,3,1011470035,1,01147,0035,104,WEST 76 STREET,10023,990505,223527,40.780203\n"
     ]
    }
   ],
   "source": [
    "print(infile.read(100)) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is the file connection closed? True\n"
     ]
    }
   ],
   "source": [
    "# need to remember to close the file when we are done\n",
    "infile.close()\n",
    "print('is the file connection closed?', infile.closed) # it had better be!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the `readline()` function: \n",
    "- reads/fetches one line.\n",
    "- returns a single string \n",
    "- if size is specified, at most size characters will be read.\n",
    "- moves the read head forward to the next line\n",
    "- if the line being read is blank (but not a the last line), `readline()` returns '\\n',\n",
    "- `infile.readline()` returns an empty string if and only if the end of the file has been reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSPECTION_TYPE,JOB_TICKET_OR_WORK_ORDER_ID,JOB_ID,JOB_PROGRESS,BBL,BORO_CODE,BLOCK,LOT,HOUSE_NUMBER,STREET_NAME,ZIP_CODE,X_COORD,Y_COORD,LATITUDE,LONGITUDE,BOROUGH,INSPECTION_DATE,RESULT,APPROVED_DATE,LOCATION\n",
      "\n",
      "BAIT,1,PO12965,3,1011470035,1,01147,0035,104,WEST 76 STREET,10023,990505,223527,40.7802039792471,-73.9774144709456,Manhattan,10/14/2009 12:00:27 PM,Bait applied,10/14/2009 03:01:46 PM,\"(40.7802039792471, -73.9774144709456)\"\n",
      "\n",
      "BAIT,2,PO12966,3,1011470034,1,01147,0034,102,WEST \n"
     ]
    }
   ],
   "source": [
    "infile = open(filename_csv) \n",
    "print(infile.readline())  # first line, a header\n",
    "print(infile.readline())  # second line \n",
    "print(infile.readline(50)) # first 50 characters of third line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to remember to close the file when we are done\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> INSPECTION_TYPE,JOB_TICKET_OR_WORK_ORDER_ID,JOB_ID,JOB_PROGRESS,BBL,BORO_CODE,BLOCK,LOT,HOUSE_NUMBER,STREET_NAME,ZIP_CODE,X_COORD,Y_COORD,LATITUDE,LONGITUDE,BOROUGH,INSPECTION_DATE,RESULT,APPROVED_DATE,LOCATION\n",
      "\n",
      "> BAIT,1,PO12965,3,1011470035,1,01147,0035,104,WEST 76 STREET,10023,990505,223527,40.7802039792471,-73.9774144709456,Manhattan,10/14/2009 12:00:27 PM,Bait applied,10/14/2009 03:01:46 PM,\"(40.7802039792471, -73.9774144709456)\"\n",
      "\n",
      "> BAIT,2,PO12966,3,1011470034,1,01147,0034,102,WEST 76 STREET,10023,990516,223521,40.7801875030438,-73.977374757787,Manhattan,10/14/2009 12:51:21 PM,Bait applied,10/14/2009 03:02:30 PM,\"(40.7801875030438, -73.977374757787)\"\n",
      "\n",
      "> BAIT,30,PO16966,3,2043370027,2,04337,0027,620,THWAITES PLACE,10467,1020110,252216,40.8588765781972,-73.8703636422023,Bronx,11/09/2009 12:59:55 PM,Bait applied,11/10/2009 02:54:52 PM,\"(40.8588765781972, -73.8703636422023)\"\n",
      "\n",
      "> BAIT,31,PO13665,3,2037670077,2,03767,0077,1227,WHITEPLAINS ROAD,10472,1022441,242180,40.8313209626148,-73.861994089899,Bronx,11/09/2009 11:10:16 AM,Bait applied,11/10/2009 02:56:42 PM,\"(40.8313209626148, -73.861994089899)\"\n",
      "\n",
      "> BAIT,38,PO11291,3,1011690057,1,01169,0057,2199,BROADWAY,10024,989641,224567,40.7830590725833,-73.9805333640688,Manhattan,11/10/2009 08:40:42 AM,Bait applied,11/17/2009 11:39:11 AM,\"(40.7830590725833, -73.9805333640688)\"\n",
      "\n",
      "is the file connection closed? True\n"
     ]
    }
   ],
   "source": [
    "# the file handler can be iterated over\n",
    "infile = open(filename_csv, mode='r') # explicitly setting the mode.\n",
    "for line in infile:\n",
    "    print('>', line)\n",
    "infile.close()\n",
    "print('is the file connection closed?', infile.closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row number: 0 : INSPECTION_TYPE,JOB_TICKET_OR_WORK_ORDER_ID,JOB_ID,JOB_PROGRESS,BBL,BORO_CODE,BLOCK,LOT,HOUSE_NUMBER,STREET_NAME,ZIP_CODE,X_COORD,Y_COORD,LATITUDE,LONGITUDE,BOROUGH,INSPECTION_DATE,RESULT,APPROVED_DATE,LOCATION\n",
      "\n",
      "row number: 1 : BAIT,1,PO12965,3,1011470035,1,01147,0035,104,WEST 76 STREET,10023,990505,223527,40.7802039792471,-73.9774144709456,Manhattan,10/14/2009 12:00:27 PM,Bait applied,10/14/2009 03:01:46 PM,\"(40.7802039792471, -73.9774144709456)\"\n",
      "\n",
      "row number: 2 : BAIT,2,PO12966,3,1011470034,1,01147,0034,102,WEST 76 STREET,10023,990516,223521,40.7801875030438,-73.977374757787,Manhattan,10/14/2009 12:51:21 PM,Bait applied,10/14/2009 03:02:30 PM,\"(40.7801875030438, -73.977374757787)\"\n",
      "\n",
      "row number: 3 : BAIT,30,PO16966,3,2043370027,2,04337,0027,620,THWAITES PLACE,10467,1020110,252216,40.8588765781972,-73.8703636422023,Bronx,11/09/2009 12:59:55 PM,Bait applied,11/10/2009 02:54:52 PM,\"(40.8588765781972, -73.8703636422023)\"\n",
      "\n",
      "row number: 4 : BAIT,31,PO13665,3,2037670077,2,03767,0077,1227,WHITEPLAINS ROAD,10472,1022441,242180,40.8313209626148,-73.861994089899,Bronx,11/09/2009 11:10:16 AM,Bait applied,11/10/2009 02:56:42 PM,\"(40.8313209626148, -73.861994089899)\"\n",
      "\n",
      "row number: 5 : BAIT,38,PO11291,3,1011690057,1,01169,0057,2199,BROADWAY,10024,989641,224567,40.7830590725833,-73.9805333640688,Manhattan,11/10/2009 08:40:42 AM,Bait applied,11/17/2009 11:39:11 AM,\"(40.7830590725833, -73.9805333640688)\"\n",
      "\n",
      "is the file connection closed? True\n"
     ]
    }
   ],
   "source": [
    "# here is a common way to iterate over the lines:\n",
    "with open(filename_csv, mode='r') as infile:\n",
    "    # the enumerate function returns an iterable\n",
    "    for i, line in enumerate(infile):\n",
    "        # process each row?\n",
    "        print('row number:', i, ':', line)\n",
    "# this way, the file connection is automatically closed when we exit the loop!\n",
    "print('is the file connection closed?', infile.closed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now you can start to see how we **could** use this to extract data from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'INSPECTION_TYPE': 'BAIT', 'JOB_TICKET_OR_WORK_ORDER_ID': '1', 'JOB_ID': 'PO12965', 'JOB_PROGRESS': '3', 'BBL': '1011470035', 'BORO_CODE': '1', 'BLOCK': '01147', 'LOT': '0035', 'HOUSE_NUMBER': '104', 'STREET_NAME': 'WEST 76 STREET', 'ZIP_CODE': '10023', 'X_COORD': '990505', 'Y_COORD': '223527', 'LATITUDE': '40.7802039792471', 'LONGITUDE': '-73.9774144709456', 'BOROUGH': 'Manhattan', 'INSPECTION_DATE': '10/14/2009 12:00:27 PM', 'RESULT': 'Bait applied', 'APPROVED_DATE': '10/14/2009 03:01:46 PM', 'LOCATION\\n': '\"(40.7802039792471'}, {'INSPECTION_TYPE': 'BAIT', 'JOB_TICKET_OR_WORK_ORDER_ID': '2', 'JOB_ID': 'PO12966', 'JOB_PROGRESS': '3', 'BBL': '1011470034', 'BORO_CODE': '1', 'BLOCK': '01147', 'LOT': '0034', 'HOUSE_NUMBER': '102', 'STREET_NAME': 'WEST 76 STREET', 'ZIP_CODE': '10023', 'X_COORD': '990516', 'Y_COORD': '223521', 'LATITUDE': '40.7801875030438', 'LONGITUDE': '-73.977374757787', 'BOROUGH': 'Manhattan', 'INSPECTION_DATE': '10/14/2009 12:51:21 PM', 'RESULT': 'Bait applied', 'APPROVED_DATE': '10/14/2009 03:02:30 PM', 'LOCATION\\n': '\"(40.7801875030438'}, {'INSPECTION_TYPE': 'BAIT', 'JOB_TICKET_OR_WORK_ORDER_ID': '30', 'JOB_ID': 'PO16966', 'JOB_PROGRESS': '3', 'BBL': '2043370027', 'BORO_CODE': '2', 'BLOCK': '04337', 'LOT': '0027', 'HOUSE_NUMBER': '620', 'STREET_NAME': 'THWAITES PLACE', 'ZIP_CODE': '10467', 'X_COORD': '1020110', 'Y_COORD': '252216', 'LATITUDE': '40.8588765781972', 'LONGITUDE': '-73.8703636422023', 'BOROUGH': 'Bronx', 'INSPECTION_DATE': '11/09/2009 12:59:55 PM', 'RESULT': 'Bait applied', 'APPROVED_DATE': '11/10/2009 02:54:52 PM', 'LOCATION\\n': '\"(40.8588765781972'}, {'INSPECTION_TYPE': 'BAIT', 'JOB_TICKET_OR_WORK_ORDER_ID': '31', 'JOB_ID': 'PO13665', 'JOB_PROGRESS': '3', 'BBL': '2037670077', 'BORO_CODE': '2', 'BLOCK': '03767', 'LOT': '0077', 'HOUSE_NUMBER': '1227', 'STREET_NAME': 'WHITEPLAINS ROAD', 'ZIP_CODE': '10472', 'X_COORD': '1022441', 'Y_COORD': '242180', 'LATITUDE': '40.8313209626148', 'LONGITUDE': '-73.861994089899', 'BOROUGH': 'Bronx', 'INSPECTION_DATE': '11/09/2009 11:10:16 AM', 'RESULT': 'Bait applied', 'APPROVED_DATE': '11/10/2009 02:56:42 PM', 'LOCATION\\n': '\"(40.8313209626148'}, {'INSPECTION_TYPE': 'BAIT', 'JOB_TICKET_OR_WORK_ORDER_ID': '38', 'JOB_ID': 'PO11291', 'JOB_PROGRESS': '3', 'BBL': '1011690057', 'BORO_CODE': '1', 'BLOCK': '01169', 'LOT': '0057', 'HOUSE_NUMBER': '2199', 'STREET_NAME': 'BROADWAY', 'ZIP_CODE': '10024', 'X_COORD': '989641', 'Y_COORD': '224567', 'LATITUDE': '40.7830590725833', 'LONGITUDE': '-73.9805333640688', 'BOROUGH': 'Manhattan', 'INSPECTION_DATE': '11/10/2009 08:40:42 AM', 'RESULT': 'Bait applied', 'APPROVED_DATE': '11/17/2009 11:39:11 AM', 'LOCATION\\n': '\"(40.7830590725833'}]\n",
      "is the file connection closed? True\n"
     ]
    }
   ],
   "source": [
    "with open(filename_csv, mode='r') as infile:\n",
    "    headerline_str = infile.readline()  # first line, a header\n",
    "    header = headerline_str.split(',')\n",
    "    data_table = []\n",
    "    for line in infile:\n",
    "        values = line.split(',')\n",
    "        row_dict = dict(zip(header, values))\n",
    "        data_table.append(row_dict)\n",
    "print(data_table)\n",
    "print('is the file connection closed?', infile.closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10023', '10023', '10467', '10472', '10024']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we could access individual elements from the data file:\n",
    "[arow['ZIP_CODE'] for arow in data_table]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "don't worry. we won't be doing much of that. as we will see, there are better ways. but let's continue exploring the basic functions. by the way, did you notice a big flaw with the above data table? what if you look consider this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "header has 20 elements, ending with LOCATION\n",
      "\n",
      "data line has 21 elements, ending with  -73.9774144709456)\"\n",
      "\n",
      "data line has 21 elements, ending with  -73.977374757787)\"\n",
      "\n",
      "data line has 21 elements, ending with  -73.8703636422023)\"\n",
      "\n",
      "data line has 21 elements, ending with  -73.861994089899)\"\n",
      "\n",
      "data line has 21 elements, ending with  -73.9805333640688)\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(filename_csv, 'r') as infile:\n",
    "    headerline_str = infile.readline()  # first line, a header\n",
    "    header = headerline_str.split(',')\n",
    "    print('header has', len(header), 'elements, ending with', header[-1])\n",
    "    for line in infile:\n",
    "        values = line.split(',')\n",
    "        print('data line has', len(values), 'elements, ending with', values[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "files labelled as csv files can differ by a lot. especially when they contain fields that are free-texts (like mentions do!). csv files you will find in the wild can contain various idiosyncracies:\n",
    "- they may be tab-delimited, pipe delimited, semi-colon-delimited,...\n",
    "- they may come from foreign systems that use a different from default end of line pattern ('\\r\\n')\n",
    "- they may include quoted fields that contain the delimeter a,b,\"bla, bla, bla\", 'how many fields in this line?'\n",
    "- they may contain fields containing end-of-line characters.\n",
    "- they may contain various control characters that are 'escaped'.\n",
    "\n",
    "processing a csv file like this is possible, and even easy. but it is laborious. so, there is a module for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the `csv` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row number 1 has 20 fields and ends on LOCATION\n",
      "row number 2 has 20 fields and ends on (40.7802039792471, -73.9774144709456)\n",
      "row number 3 has 20 fields and ends on (40.7801875030438, -73.977374757787)\n",
      "row number 4 has 20 fields and ends on (40.8588765781972, -73.8703636422023)\n",
      "row number 5 has 20 fields and ends on (40.8313209626148, -73.861994089899)\n",
      "row number 6 has 20 fields and ends on (40.7830590725833, -73.9805333640688)\n"
     ]
    }
   ],
   "source": [
    "# simple usage:\n",
    "with open(filename_csv, newline='') as infile:\n",
    "    # we first use the csv.reader() function to create a reader object\n",
    "    rowreader = csv.reader(infile)\n",
    "    # we can iterate over the rowreader\n",
    "    for row in rowreader:\n",
    "        # row is already a list of data values!\n",
    "        num_vals = len(row)\n",
    "        print('row number', rowreader.line_num, 'has', num_vals, 'fields and ends on', row[num_vals-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excel', 'excel-tab', 'unix']\n",
      "row number 1 has 20 fields and ends on LOCATION\n",
      "row number 2 has 20 fields and ends on (40.7802039792471, -73.9774144709456)\n",
      "row number 3 has 20 fields and ends on (40.7801875030438, -73.977374757787)\n",
      "row number 4 has 20 fields and ends on (40.8588765781972, -73.8703636422023)\n",
      "row number 5 has 20 fields and ends on (40.8313209626148, -73.861994089899)\n",
      "row number 6 has 20 fields and ends on (40.7830590725833, -73.9805333640688)\n"
     ]
    }
   ],
   "source": [
    "# the csv module comes with distinct dialects for parsing csv files:\n",
    "print(csv.list_dialects())\n",
    "# we can specify which dialect we want to use:\n",
    "with open(filename_csv, newline='') as infile:\n",
    "    # we first use the csv.reader() function to create a reader object\n",
    "    rowreader = csv.reader(infile, dialect='unix')\n",
    "    # we can iterate over the rowreader\n",
    "    for row in rowreader:\n",
    "        # row is already a list of data values!\n",
    "        num_vals = len(row)\n",
    "        print('row number', rowreader.line_num, 'has', num_vals, 'fields and ends on', row[num_vals-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if our csv file has a different structure than any of the three given dialects, we can create our own. the `csv` module contains a class called `Sniffer` for helping with that by inspecting our csv file for the structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file datafiles/NY_rodent_inspections_sample_small.csv has a header.\n",
      "the csv file datafiles/NY_rodent_inspections_sample_small.csv has \",\" as its delimiter character\n",
      "the csv file datafiles/NY_rodent_inspections_sample_small.csv has None escapechar character\n",
      "if a quotechar occurs inside a field, it gets escaped by None\n",
      "the csv file datafiles/NY_rodent_inspections_sample_small.csv has \" as its quoting char\n",
      "the csv file datafiles/NY_rodent_inspections_sample_small.csv has lineterminator: slash-r-slash-n\n",
      "the csv file datafiles/NY_rodent_inspections_sample_small.csv has quote level 0\n",
      "the csv file datafiles/NY_rodent_inspections_sample_small.csv respects whitespace characters following the delimiter\n"
     ]
    }
   ],
   "source": [
    "with open(filename_csv, newline='') as infile:\n",
    "    # use the regular read() to fetch a sample of the file\n",
    "    sample = infile.read(2048) \n",
    "    inspector = csv.Sniffer() # generate an inspection object\n",
    "    # does there seem to be a header?\n",
    "    is_there_a_header = inspector.has_header(sample) \n",
    "    # reveal the dialect discovered:\n",
    "    if is_there_a_header:\n",
    "        print('file', filename_csv, 'has a header.')\n",
    "    else:\n",
    "        print('file', filename_csv, 'has no header!')\n",
    "\n",
    "\n",
    "with open(filename_csv, newline='') as infile:\n",
    "    sample = infile.read(2048) \n",
    "    inspector = csv.Sniffer() # generate an inspection object\n",
    "    # guess the dialect of the csv file: delimiter, quote characters, etc\n",
    "    dialect = inspector.sniff(sample)\n",
    "\n",
    "    print('the csv file', filename_csv, 'has \"' + dialect.delimiter + '\" as its delimiter character')\n",
    "    if dialect.escapechar is not None:\n",
    "        print('the csv file', filename_csv, 'has \"'+str(dialect.escapechar)+'as its escapechar char')\n",
    "    else:\n",
    "        print('the csv file', filename_csv, 'has None escapechar character')\n",
    "    if dialect.doublequote:\n",
    "        # controls how instances of quotechar appearing inside a field should themselves be quoted.\n",
    "        # when True, the character is doubled. when False, the escapechar is used as a prefix to \n",
    "        # the quotechar. defaults to True.\n",
    "        print('if a quotechar occurs inside a field, it gets doubled')\n",
    "    else:\n",
    "        print('if a quotechar occurs inside a field, it gets escaped by', str(dialect.escapechar))\n",
    "    print('the csv file', filename_csv, 'has', dialect.quotechar, 'as its', 'quoting char')\n",
    "    if dialect.lineterminator is None:\n",
    "        print('the csv file', filename_csv, 'has lineterminator: None')\n",
    "    elif dialect.lineterminator == '\\r':\n",
    "        print('the csv file', filename_csv, 'has lineterminator: slash-r')\n",
    "    elif dialect.lineterminator == '\\n':\n",
    "        print('the csv file', filename_csv, 'has lineterminator: slash-n')\n",
    "    elif dialect.lineterminator == '\\r\\n':\n",
    "        print('the csv file', filename_csv, 'has lineterminator: slash-r-slash-n')\n",
    "    elif dialect.lineterminator == '':\n",
    "        print('the csv file', filename_csv, 'has lineterminator: blank')\n",
    "    else:\n",
    "        print('the csv file', filename_csv, 'has lineterminator: '+ str(dialect.lineterminator))\n",
    "    \n",
    "    if dialect.quoting is not None:\n",
    "        # dialect.quoting controls when quotes should be generated by the writer and recognised by the reader. \n",
    "        # It can take on any of the QUOTE_* constants and defaults to QUOTE_MINIMAL.\n",
    "        #  [QUOTE_ALL, QUOTE_MINIMAL, QUOTE_NONNUMERIC, QUOTE_NONE]\n",
    "        print('the csv file', filename_csv, 'has quote level', str(dialect.quoting))\n",
    "    if dialect.skipinitialspace:\n",
    "        print('the csv file', filename_csv, 'ignores whitespace characters following the delimiter')\n",
    "    else:\n",
    "        print('the csv file', filename_csv, 'respects whitespace characters following the delimiter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file datafiles/iris.csv has a header.\n",
      "the csv file datafiles/iris.csv has \",\" as its delimiter charcter\n",
      "the csv file datafiles/iris.csv has None escapechar character\n",
      "if a quotechar occurs inside a field, it gets escaped by None\n",
      "the csv file datafiles/NY_rodent_inspections_sample_small.csv has 0 as its quoting char\n",
      "the csv file datafiles/iris.csv has lineterminator: slash-r-slash-n\n",
      "the csv file datafiles/iris.csv has quote level 0\n",
      "the csv file datafiles/iris.csv respects whitespace characters immediately following the delimiter\n"
     ]
    }
   ],
   "source": [
    "def report_on_dialect(filename, dialect):\n",
    "    # report on the dialect discovered:\n",
    "    # print(dialect)\n",
    "    print('the csv file', filename, 'has \"' + dialect.delimiter + '\" as its delimiter charcter')\n",
    "    if dialect.escapechar is not None:\n",
    "        print('the csv file', filename, 'has \"'+str(dialect.escapechar)+'as its escapechar char')\n",
    "    else: \n",
    "        print('the csv file', filename, 'has None escapechar character')\n",
    "    if dialect.doublequote:\n",
    "        # Controls how instances of quotechar appearing inside a field should themselves be quoted.\n",
    "        # When True, the character is doubled. \n",
    "        # When False, the escapechar is used as a prefix to the quotechar. It defaults to True.\n",
    "        print('if a quotechar occurs inside a field, it gets doubled')\n",
    "    else:\n",
    "        print('if a quotechar occurs inside a field, it gets escaped by '+ str(dialect.escapechar))\n",
    "    \n",
    "    print('the csv file', filename_csv, 'has', dialect.quoting,           'as its', 'quoting char')\n",
    "    \n",
    "    if dialect.lineterminator is None:\n",
    "        print('the csv file', filename, 'has lineterminator: None')\n",
    "    elif dialect.lineterminator == '\\r':\n",
    "        print('the csv file', filename, 'has lineterminator: slash-r')\n",
    "    elif dialect.lineterminator == '\\n':\n",
    "        print('the csv file', filename, 'has lineterminator: slash-n')\n",
    "    elif dialect.lineterminator == '\\r\\n':\n",
    "        print('the csv file', filename, 'has lineterminator: slash-r-slash-n')\n",
    "    elif dialect.lineterminator == '':\n",
    "        print('the csv file', filename, 'has lineterminator: blank')\n",
    "    else:\n",
    "        print('the csv file', filename, 'has lineterminator: '+ str(dialect.lineterminator))\n",
    "    \n",
    "    if dialect.quoting is not None:\n",
    "        # dialect.quoting controls when quotes should be generated by the writer and recognised by the reader. \n",
    "        # It can take on any of the QUOTE_* constants and defaults to QUOTE_MINIMAL.\n",
    "        #  [QUOTE_ALL, QUOTE_MINIMAL, QUOTE_NONNUMERIC, QUOTE_NONE]\n",
    "        print('the csv file', filename, 'has quote level', str(dialect.quoting))\n",
    "    if dialect.skipinitialspace:\n",
    "        print('the csv file', filename, 'ignores whitespace characters immediately following the delimiter')\n",
    "    else:\n",
    "        print('the csv file', filename, 'respects whitespace characters immediately following the delimiter')\n",
    "\n",
    "def inspect_csv(filename, sample_size=2048):\n",
    "    with open(filename, newline='') as infile:\n",
    "        sample = infile.read(sample_size) # fetch the first few rows\n",
    "        infile.seek(0) # rewind\n",
    "        sniffer = csv.Sniffer() # inspection class for csv files\n",
    "        # does there seem to be a header?\n",
    "        is_there_a_header = sniffer.has_header(sample) \n",
    "        if is_there_a_header:\n",
    "            print('file', filename, 'has a header.')\n",
    "        else:\n",
    "            print('file', filename, 'has no header!')\n",
    "\n",
    "        # guess the delimiter, quote characters\n",
    "        dialect = sniffer.sniff(sample)\n",
    "        report_on_dialect(filename, dialect)\n",
    "\n",
    "inspect_csv('datafiles/iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSPECTION_TYPE\n",
      "BAIT\n",
      "BAIT\n",
      "BAIT\n",
      "BAIT\n",
      "BAIT\n",
      "Processed 5 lines.\n",
      "header: ['INSPECTION_TYPE', 'JOB_TICKET_OR_WORK_ORDER_ID', 'JOB_ID', 'JOB_PROGRESS', 'BBL', 'BORO_CODE', 'BLOCK', 'LOT', 'HOUSE_NUMBER', 'STREET_NAME', 'ZIP_CODE', 'X_COORD', 'Y_COORD', 'LATITUDE', 'LONGITUDE', 'BOROUGH', 'INSPECTION_DATE', 'RESULT', 'APPROVED_DATE', 'LOCATION']\n"
     ]
    }
   ],
   "source": [
    "with open(filename_csv) as infile:\n",
    "    rowreader = csv.reader(infile, delimiter=',')\n",
    "    for line_num, row in enumerate(rowreader):\n",
    "        if line_num == 0:\n",
    "            # first line is a header\n",
    "            column_header = row\n",
    "        print(row[0])\n",
    "    print(f'Processed {line_num} lines.')\n",
    "    print('header:', column_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading csv files to a dict\n",
    "what the best way to read in a data file can depend on what we want to do with it next. getting the rows as a `list` object may not be the most useful. instead, you may want a dictionary whose keys are the column heads, and whose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row number 0 has 20 columns\n",
      "row number 1 has 20 columns\n",
      "row number 2 has 20 columns\n",
      "row number 3 has 20 columns\n",
      "row number 4 has 20 columns\n",
      "read in 5 rows from datafiles/NY_rodent_inspections_sample_small.csv\n"
     ]
    }
   ],
   "source": [
    "with open(filename_csv, mode='r') as infile:\n",
    "    # the csv offers another reader function for generating dicts:\n",
    "    rowreader = csv.DictReader(infile)\n",
    "    data_table = []\n",
    "    for rownum, row in enumerate(rowreader):\n",
    "        # now every row is a dictionary keyed on the values of the first row\n",
    "        print('row number', rownum, 'has', len(row), 'columns')\n",
    "        data_table.append(row)\n",
    "print('read in', len(data_table), 'rows from', filename_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10023', '10023', '10467', '10472', '10024']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we can access any column or value directly\n",
    "[arow['ZIP_CODE'] for arow in data_table]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(40.7802039792471, -73.9774144709456)',\n",
       " '(40.7801875030438, -73.977374757787)',\n",
       " '(40.8588765781972, -73.8703636422023)',\n",
       " '(40.8313209626148, -73.861994089899)',\n",
       " '(40.7830590725833, -73.9805333640688)']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[arow['LOCATION'] for arow in data_table]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compressed csv files\n",
    "we often deal with large data files. often these are shipped and stored in a compressed format, most often gnu zipped (.gz). the good news is that we can read zipped files directly and easily by changing only the opener! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in 9999 rows from datafiles/NY_rodent_inspections_sample_small.csv\n",
      "last 5 locations:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['(40.7077744363728, -73.8017444698633)',\n",
       " '(40.8788397683361, -73.9017406808259)',\n",
       " '(40.8549066197023, -73.9130302748932)',\n",
       " '(40.8293887213765, -73.9121234267685)',\n",
       " '(40.8460651323037, -73.9115881298086)']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "filename_zipped='datafiles/NY_rodent_inspections_sample.csv.gz'\n",
    "with gzip.open(filename_zipped, mode=\"rt\") as infile:\n",
    "    # eerything else is exactly the same:\n",
    "    rowreader = csv.DictReader(infile)\n",
    "    data_table = []\n",
    "    for rownum, row in enumerate(rowreader):\n",
    "        data_table.append(row)\n",
    "print('read in', len(data_table), 'rows from', filename_csv)\n",
    "print('last 5 locations:')\n",
    "[arow['LOCATION'] for arow in data_table][-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that in engineering, statistics, and physical science, it is common to insert comments into csv files. there [currently is no comment overriding mechanism](https://bugs.python.org/issue1225769) built in to the csv module. one could devise a solution using `str.startswith().`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INSPECTION_TYPE', 'JOB_TICKET_OR_WORK_ORDER_ID', 'JOB_ID', 'JOB_PROGRESS', 'BBL', 'BORO_CODE', 'BLOCK', 'LOT', 'HOUSE_NUMBER', 'STREET_NAME', 'ZIP_CODE', 'X_COORD', 'Y_COORD', 'LATITUDE', 'LONGITUDE', 'BOROUGH', 'INSPECTION_DATE', 'RESULT', 'APPROVED_DATE', 'LOCATION']\n",
      "['BAIT', '1', 'PO12965', '3', '1011470035', '1', '01147', '0035', '104', 'WEST 76 STREET', '10023', '990505', '223527', '40.7802039792471', '-73.9774144709456', 'Manhattan', '10/14/2009 12:00:27 PM', 'Bait applied', '10/14/2009 03:01:46 PM', '(40.7802039792471, -73.9774144709456)']\n",
      "['BAIT', '2', 'PO12966', '3', '1011470034', '1', '01147', '0034', '102', 'WEST 76 STREET', '10023', '990516', '223521', '40.7801875030438', '-73.977374757787', 'Manhattan', '10/14/2009 12:51:21 PM', 'Bait applied', '10/14/2009 03:02:30 PM', '(40.7801875030438, -73.977374757787)']\n",
      "['BAIT', '30', 'PO16966', '3', '2043370027', '2', '04337', '0027', '620', 'THWAITES PLACE', '10467', '1020110', '252216', '40.8588765781972', '-73.8703636422023', 'Bronx', '11/09/2009 12:59:55 PM', 'Bait applied', '11/10/2009 02:54:52 PM', '(40.8588765781972, -73.8703636422023)']\n",
      "['BAIT', '31', 'PO13665', '3', '2037670077', '2', '03767', '0077', '1227', 'WHITEPLAINS ROAD', '10472', '1022441', '242180', '40.8313209626148', '-73.861994089899', 'Bronx', '11/09/2009 11:10:16 AM', 'Bait applied', '11/10/2009 02:56:42 PM', '(40.8313209626148, -73.861994089899)']\n",
      "['BAIT', '38', 'PO11291', '3', '1011690057', '1', '01169', '0057', '2199', 'BROADWAY', '10024', '989641', '224567', '40.7830590725833', '-73.9805333640688', 'Manhattan', '11/10/2009 08:40:42 AM', 'Bait applied', '11/17/2009 11:39:11 AM', '(40.7830590725833, -73.9805333640688)']\n"
     ]
    }
   ],
   "source": [
    "def decomment(csvfile):\n",
    "    for row in csvfile:\n",
    "        raw = row.split('#')[0].strip()\n",
    "        if raw: yield raw\n",
    "\n",
    "with open(filename_csv) as csvfile:\n",
    "    reader = csv.reader(decomment(csvfile))\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a recommendation: use csvkit!\n",
    "finally, for csv files: look into using [csvkit]\n",
    "(https://csvkit.readthedocs.io/en/1.0.3/tutorial/1_getting_started.html) a collection of utilities for working with csv from the command line, and fixing csv issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the `pandas` module\n",
    "the pandas module is a generic data science module of python that has a lot of functions. we will talk about it in detail later, but for now just note how easy it makes reading and writing csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas offers the `read_csv()` function that takes in a name of a tidy csv file and returns a dataframe (a data frame is a table-like structure of columns and rows of tidy data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rodent_df = pd.read_csv(filename_csv) # yeah. that's it. neat, huh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd.read_csv()` opens, analyzes, reads, parses the csv file and arranges the data into a pd data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rodent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INSPECTION_TYPE</th>\n",
       "      <th>JOB_TICKET_OR_WORK_ORDER_ID</th>\n",
       "      <th>JOB_ID</th>\n",
       "      <th>JOB_PROGRESS</th>\n",
       "      <th>BBL</th>\n",
       "      <th>BORO_CODE</th>\n",
       "      <th>BLOCK</th>\n",
       "      <th>LOT</th>\n",
       "      <th>HOUSE_NUMBER</th>\n",
       "      <th>STREET_NAME</th>\n",
       "      <th>ZIP_CODE</th>\n",
       "      <th>X_COORD</th>\n",
       "      <th>Y_COORD</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>BOROUGH</th>\n",
       "      <th>INSPECTION_DATE</th>\n",
       "      <th>RESULT</th>\n",
       "      <th>APPROVED_DATE</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BAIT</td>\n",
       "      <td>1</td>\n",
       "      <td>PO12965</td>\n",
       "      <td>3</td>\n",
       "      <td>1011470035</td>\n",
       "      <td>1</td>\n",
       "      <td>1147</td>\n",
       "      <td>35</td>\n",
       "      <td>104</td>\n",
       "      <td>WEST 76 STREET</td>\n",
       "      <td>10023</td>\n",
       "      <td>990505</td>\n",
       "      <td>223527</td>\n",
       "      <td>40.780204</td>\n",
       "      <td>-73.977414</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>10/14/2009 12:00:27 PM</td>\n",
       "      <td>Bait applied</td>\n",
       "      <td>10/14/2009 03:01:46 PM</td>\n",
       "      <td>(40.7802039792471, -73.9774144709456)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BAIT</td>\n",
       "      <td>2</td>\n",
       "      <td>PO12966</td>\n",
       "      <td>3</td>\n",
       "      <td>1011470034</td>\n",
       "      <td>1</td>\n",
       "      <td>1147</td>\n",
       "      <td>34</td>\n",
       "      <td>102</td>\n",
       "      <td>WEST 76 STREET</td>\n",
       "      <td>10023</td>\n",
       "      <td>990516</td>\n",
       "      <td>223521</td>\n",
       "      <td>40.780188</td>\n",
       "      <td>-73.977375</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>10/14/2009 12:51:21 PM</td>\n",
       "      <td>Bait applied</td>\n",
       "      <td>10/14/2009 03:02:30 PM</td>\n",
       "      <td>(40.7801875030438, -73.977374757787)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAIT</td>\n",
       "      <td>30</td>\n",
       "      <td>PO16966</td>\n",
       "      <td>3</td>\n",
       "      <td>2043370027</td>\n",
       "      <td>2</td>\n",
       "      <td>4337</td>\n",
       "      <td>27</td>\n",
       "      <td>620</td>\n",
       "      <td>THWAITES PLACE</td>\n",
       "      <td>10467</td>\n",
       "      <td>1020110</td>\n",
       "      <td>252216</td>\n",
       "      <td>40.858877</td>\n",
       "      <td>-73.870364</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>11/09/2009 12:59:55 PM</td>\n",
       "      <td>Bait applied</td>\n",
       "      <td>11/10/2009 02:54:52 PM</td>\n",
       "      <td>(40.8588765781972, -73.8703636422023)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BAIT</td>\n",
       "      <td>31</td>\n",
       "      <td>PO13665</td>\n",
       "      <td>3</td>\n",
       "      <td>2037670077</td>\n",
       "      <td>2</td>\n",
       "      <td>3767</td>\n",
       "      <td>77</td>\n",
       "      <td>1227</td>\n",
       "      <td>WHITEPLAINS ROAD</td>\n",
       "      <td>10472</td>\n",
       "      <td>1022441</td>\n",
       "      <td>242180</td>\n",
       "      <td>40.831321</td>\n",
       "      <td>-73.861994</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>11/09/2009 11:10:16 AM</td>\n",
       "      <td>Bait applied</td>\n",
       "      <td>11/10/2009 02:56:42 PM</td>\n",
       "      <td>(40.8313209626148, -73.861994089899)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BAIT</td>\n",
       "      <td>38</td>\n",
       "      <td>PO11291</td>\n",
       "      <td>3</td>\n",
       "      <td>1011690057</td>\n",
       "      <td>1</td>\n",
       "      <td>1169</td>\n",
       "      <td>57</td>\n",
       "      <td>2199</td>\n",
       "      <td>BROADWAY</td>\n",
       "      <td>10024</td>\n",
       "      <td>989641</td>\n",
       "      <td>224567</td>\n",
       "      <td>40.783059</td>\n",
       "      <td>-73.980533</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>11/10/2009 08:40:42 AM</td>\n",
       "      <td>Bait applied</td>\n",
       "      <td>11/17/2009 11:39:11 AM</td>\n",
       "      <td>(40.7830590725833, -73.9805333640688)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  INSPECTION_TYPE  JOB_TICKET_OR_WORK_ORDER_ID   JOB_ID  JOB_PROGRESS  \\\n",
       "0            BAIT                            1  PO12965             3   \n",
       "1            BAIT                            2  PO12966             3   \n",
       "2            BAIT                           30  PO16966             3   \n",
       "3            BAIT                           31  PO13665             3   \n",
       "4            BAIT                           38  PO11291             3   \n",
       "\n",
       "          BBL  BORO_CODE  BLOCK  LOT  HOUSE_NUMBER       STREET_NAME  \\\n",
       "0  1011470035          1   1147   35           104    WEST 76 STREET   \n",
       "1  1011470034          1   1147   34           102    WEST 76 STREET   \n",
       "2  2043370027          2   4337   27           620    THWAITES PLACE   \n",
       "3  2037670077          2   3767   77          1227  WHITEPLAINS ROAD   \n",
       "4  1011690057          1   1169   57          2199          BROADWAY   \n",
       "\n",
       "   ZIP_CODE  X_COORD  Y_COORD   LATITUDE  LONGITUDE    BOROUGH  \\\n",
       "0     10023   990505   223527  40.780204 -73.977414  Manhattan   \n",
       "1     10023   990516   223521  40.780188 -73.977375  Manhattan   \n",
       "2     10467  1020110   252216  40.858877 -73.870364      Bronx   \n",
       "3     10472  1022441   242180  40.831321 -73.861994      Bronx   \n",
       "4     10024   989641   224567  40.783059 -73.980533  Manhattan   \n",
       "\n",
       "          INSPECTION_DATE        RESULT           APPROVED_DATE  \\\n",
       "0  10/14/2009 12:00:27 PM  Bait applied  10/14/2009 03:01:46 PM   \n",
       "1  10/14/2009 12:51:21 PM  Bait applied  10/14/2009 03:02:30 PM   \n",
       "2  11/09/2009 12:59:55 PM  Bait applied  11/10/2009 02:54:52 PM   \n",
       "3  11/09/2009 11:10:16 AM  Bait applied  11/10/2009 02:56:42 PM   \n",
       "4  11/10/2009 08:40:42 AM  Bait applied  11/17/2009 11:39:11 AM   \n",
       "\n",
       "                                LOCATION  \n",
       "0  (40.7802039792471, -73.9774144709456)  \n",
       "1   (40.7801875030438, -73.977374757787)  \n",
       "2  (40.8588765781972, -73.8703636422023)  \n",
       "3   (40.8313209626148, -73.861994089899)  \n",
       "4  (40.7830590725833, -73.9805333640688)  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rodent_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas' `read_csv` does have full flexibility and power when you need it:\n",
    "```\n",
    "pandas.read_csv(filepath_or_buffer, sep=', ', delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression='infer', thousands=None, decimal=b'.', lineterminator=None, quotechar='\"', quoting=0, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=None, error_bad_lines=True, warn_bad_lines=True, skipfooter=0, doublequote=True, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None)\n",
    "```\n",
    "\n",
    "we will dedicate a later session to manipulating data in a pandas dataframe. (select, filter, group by, join, reshape, etc). but now we move on to json files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the `json` module\n",
    "outside of csv, another very common data format is json. not coincidentally, this hierarchical file format resembles python dicts and lists. given how flexible this format is, the data contained in a json file may not map neatly on to a flat and tidy data table.\n",
    "\n",
    "the module which will help us read json files is simply called `json`. if only life were always so simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename_json = 'datafiles/testjson.json' # small sample data in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 5 dataz from datafiles/testjson.json\n"
     ]
    }
   ],
   "source": [
    "# reading in data is simple. json.load() to read data from files and json.loads() to parse data from a string\n",
    "# https://docs.python.org/3.7/library/json.html#json.load\n",
    "# https://docs.python.org/3.7/library/json.html#json.loads\n",
    "with open(filename_json, 'r') as infile:\n",
    "    # the load method in the json module \n",
    "    data_struct = json.load(infile)\n",
    "print('read', len(data_struct), 'dataz from', filename_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"text\": \"Chicago Sun-Times\",\n",
      "    \"float\": 1.27,\n",
      "    \"datetime\": \"1948-01-01T14:57:13\",\n",
      "    \"boolean\": true,\n",
      "    \"time\": \"14:57:13\",\n",
      "    \"date\": \"1948-01-01\",\n",
      "    \"integer\": 63\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# json dump() to write to file, and dumps() to write to a string.\n",
    "# https://docs.python.org/3.7/library/json.html#json.dump\n",
    "# https://docs.python.org/3.7/library/json.html#json.dumps\n",
    "print(json.dumps(data_struct[1], sort_keys=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicago Sun-Times\n"
     ]
    }
   ],
   "source": [
    "print(data_struct[1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"a\": 10,\n",
      "    \"b\": 2,\n",
      "    \"c\": 90\n",
      "}\n",
      "[1, \"simple\", \"list\"]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(dict(zip(['a','b','c'], [10,2,90])), indent=4))\n",
    "print(json.dumps([1, 'simple', 'list']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reading data from excel\n",
    "like it or loathe it, there are excel files out there, and sometimes we need to extract data from one. luckily, there is a module for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "filename_spreadsheet = 'datafiles/example_filters.xlsx' # this week's example project\n",
    "spreadsheet = load_workbook(filename_spreadsheet, read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contents of Sheet1 in datafiles/example_filters.xlsx :\n",
      "Mention Data Export\n",
      "None\n",
      "Mentions\n",
      "1 to 0 of 0\n",
      "Project Id\n",
      "1998247687\n",
      "Query Id\n",
      "[1999542744]\n",
      "Start Date\n",
      "Sat Jun 30 23:00:00 UTC 2018\n",
      "End Date\n",
      "Wed Aug 01 23:00:00 UTC 2018\n",
      "Search\n",
      "buy\n",
      "Author Group\n",
      "Test author list\n",
      "Category\n",
      "Rudeness\n",
      "Location\n",
      "[uk]\n",
      "Page Type\n",
      "[twitter]\n",
      "Sentiment\n",
      "[negative]\n",
      "Tag\n",
      "Customer Service\n",
      "Xcategory\n",
      "Slowness\n",
      "Xdomain\n",
      "www.youtube.com\n",
      "Xlocation\n",
      "[ie]\n",
      "Xpage Type\n",
      "[news, facebook]\n"
     ]
    }
   ],
   "source": [
    "# basic iterating over the cells of a spreadsheet\n",
    "for sheet in spreadsheet:\n",
    "    print('contents of', sheet.title, 'in', filename_spreadsheet, \":\")\n",
    "    for row in sheet.rows:\n",
    "        for cell in row:\n",
    "            print(cell.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openpyxl.worksheet.worksheet.Worksheet'>\n",
      "contents of Sheet 1 - iris in datafiles/iris.xlsx\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "# but if we happen know we have tidy data (each column is a measure, each row is an obervation) \n",
    "# in the excel file, we can use pandas to make it simpler:\n",
    "filename_spreadsheet = 'datafiles/iris.xlsx' # tidy data set in excel format\n",
    "spreadsheets = load_workbook(filename_spreadsheet) # simple function call\n",
    "sheet = spreadsheets.active # top level sheet.\n",
    "print(type(sheet)) \n",
    "print('contents of', sheet.title, 'in', filename_spreadsheet)\n",
    "data = sheet.values\n",
    "cols = next(data)[1:]\n",
    "data = list(data)\n",
    "idx = [r[0] for r in data]\n",
    "data = (itertools.islice(r, 1, None) for r in data)\n",
    "sheet_data_as_df = pd.DataFrame(data, index=idx, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>None</th>\n",
       "      <th>None</th>\n",
       "      <th>None</th>\n",
       "      <th>None</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sepal_length</th>\n",
       "      <td>sepal_width</td>\n",
       "      <td>petal_length</td>\n",
       "      <td>petal_width</td>\n",
       "      <td>species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.1</th>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.9</th>\n",
       "      <td>3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.7</th>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.6</th>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      NaN           NaN          NaN          NaN\n",
       "sepal_length  sepal_width  petal_length  petal_width      species\n",
       "5.1                   3.5           1.4          0.2  Iris-setosa\n",
       "4.9                     3           1.4          0.2  Iris-setosa\n",
       "4.7                   3.2           1.3          0.2  Iris-setosa\n",
       "4.6                   3.1           1.5          0.2  Iris-setosa"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet_data_as_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sepal_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sepal_length'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0360cc6bd051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msheet_data_as_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sepal_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2698\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2700\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sepal_length'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pickled python data\n",
    "\"pickled\" files are not human readable. they are compact and efficient binary files, basically a byte stream on disk. one writes them  (the write operation is called a \"dump\") and they contain exactly the data object as it is stored in the session's memory at the time of dumping. \n",
    "\n",
    "we read and write these using the `pickle` module, which works very similar to the `json` module. there are `pickle.load()` (from file) and `pickle.loads()` (from string) functions, as well as `pickle.dump()` (to file) and `pickle.dumps()` (to string).\n",
    "\n",
    "though efficient, pickle files are not a preferred data format because they cannot be opened or inspected in other programs than python, and may even not be portable from system to system. additionally, they are black boxes and might contain a hidden payload of arbitrary code, so you should be wary to import python pickle files unless you know and trust its origins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is the file there? True\n"
     ]
    }
   ],
   "source": [
    "# write a pickle file:\n",
    "import pickle\n",
    "if sheet_data_as_df:\n",
    "    # write the iris data set to a pickle file.\n",
    "    filename_pickle = 'datafiles/iris.pickle'\n",
    "    with open(filename_pickle, mode='wb') as outfile: # note the binary mode! \n",
    "        pickle.dump(sheet_data_as_df, outfile)\n",
    "\n",
    "import os\n",
    "# let us check whether the file was actually written:\n",
    "# print(os.listdir('datafiles/'))\n",
    "print('is the file there?', os.path.isfile(filename_pickle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# test reading in the file again\n",
    "with open(filename_pickle, mode='rb') as infile: # note the binary mode!\n",
    "    temp = pickle.load(infile)\n",
    "print(type(temp))\n",
    "temp.head() # the contents are exactly the same as they were."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# draft of a solution to this week's exercise\n",
    "reading in peter's problem spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "filename_spreadsheet = 'datafiles/example_filters.xlsx'\n",
    "#spreadsheet = load_workbook(filename_spreadsheet, read_only=True)\n",
    "spreadsheet = load_workbook(filename_spreadsheet) # read only files are more limited\n",
    "sheet = spreadsheet.active\n",
    "#print(type(sheet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet1\n"
     ]
    }
   ],
   "source": [
    "print(sheet.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accessing rows in sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file datafiles/example_filters/Example filters - ADAS.xlsx contains the sheet Sheet1 with Chart Data Export in the A1 cell\n"
     ]
    }
   ],
   "source": [
    "data = sheet.values\n",
    "# the data object is a generator, meaning we can step through its values with the `next()` method.\n",
    "# each call to `next()` generates a row in the form of a tuple of columns\n",
    "first_row = next(data)\n",
    "sheet_title = first_row[0] # first row, first column (cell A1) contains a title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mentions': '1 to 0 of 0', 'Project Id': '1998247687', 'Query Id': '[1999542744]', 'Start Date': 'Sat Jun 30 23:00:00 UTC 2018', 'End Date': 'Wed Aug 01 23:00:00 UTC 2018', 'Search': 'buy', 'Author Group': 'Test author list', 'Category': 'Rudeness', 'Location': '[uk]', 'Page Type': '[twitter]', 'Sentiment': '[negative]', 'Tag': 'Customer Service', 'Xcategory': 'Slowness', 'Xdomain': 'www.youtube.com', 'Xlocation': '[ie]', 'Xpage Type': '[news, facebook]'}\n"
     ]
    }
   ],
   "source": [
    "# collect the key, value pairs in the sheet into a dict\n",
    "chart_details = {}\n",
    "for row in data:\n",
    "    chart_details[row[0]] = row[1]\n",
    "print(chart_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this only works so far. some files have a lot of rubbish in the footer that needs to be ignored/cleaned\n",
    "# let's see a problematic file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file datafiles/example_filters/Example filters - Nokia.xlsx contains the sheet Sheet1 with Chart Data Export in the A1 cell\n",
      "{'Dimension 1': 'weeks', 'Dimension 2': 'queries', 'Aggregate': 'volume', 'Start Date': 'Mon Jan 01 00:00:00 UTC 2018', 'End Date': 'Mon Apr 30 23:00:00 UTC 2018', 'Project Id': '1998248269', 'Query Id': '[1999536803]', 'Search': 'nokia', 'Xdomain': 'www.youtube.com', 'Xpage Type': '[facebook]'}\n"
     ]
    }
   ],
   "source": [
    "filename_spreadsheet = 'datafiles/example_filters/Example filters - Nokia.xlsx'\n",
    "#spreadsheet = load_workbook(filename_spreadsheet, read_only=True)\n",
    "spreadsheet = load_workbook(filename_spreadsheet) # read only files are more limited\n",
    "sheet = spreadsheet.active\n",
    "data = sheet.values\n",
    "# the data object is a generator, meaning we can step through its values with the `next()` method.\n",
    "# each call to `next()` generates a row in the form of a tuple of columns\n",
    "first_row = next(data)\n",
    "sheet_title = first_row[0] # first row, first column (cell A1) contains a title.\n",
    "print(\n",
    "    'file', filename_spreadsheet,\n",
    "    'contains the sheet', sheet.title, \n",
    "    'with', sheet_title, 'in the A1 cell')\n",
    "# collect the key, value pairs in the sheet into a dict\n",
    "chart_details = {}\n",
    "for row in data:\n",
    "    if row[0] is not None and row[0] is not '':\n",
    "        chart_details[row[0]] = row[1]\n",
    "    else: \n",
    "        # stop processing data rows on first blank cell in column A:\n",
    "        break\n",
    "print(chart_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now just wrap this up as a function\n",
    "def read_chart_export(filename_spreadsheet):\n",
    "    # reads chart data export in excel format and extracts the chart data\n",
    "    # returns a dictionary with the chart properties \n",
    "    # assumes:\n",
    "    #   cell A1 contains a table title\n",
    "    #   the keys are in cells A2:An, where n is number of properties\n",
    "    #   the values are in cells B2:Bn \n",
    "    #   any rows below a blank cell in the A column are 'rubbish'\n",
    "    # nb! files are opened as read-write. because read-only files have limited functionality in the openpyxl module\n",
    "    \n",
    "    spreadsheet = load_workbook(filename_spreadsheet) \n",
    "    sheet = spreadsheet.active\n",
    "    data = sheet.values\n",
    "    # the `data` object is a generator, meaning we can step through its values with the `next()` method.\n",
    "    # each call to `next()` generates a row in the form of a tuple of columns\n",
    "    first_row = next(data)\n",
    "    sheet_title = first_row[0] # first row, first column (cell A1) contains a title.\n",
    "    print('reading exported chart data from sheet \"'+sheet_title+'\" in', filename_spreadsheet)\n",
    "    # collect the key, value pairs in the sheet into a dict\n",
    "    chart_details = {}\n",
    "    for row in data:\n",
    "        if row[0] is not None and row[0] is not '':\n",
    "            chart_details[row[0]] = row[1]\n",
    "        else: \n",
    "            # stop processing data rows on first blank cell in A column:\n",
    "            break\n",
    "    return chart_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading exported chart data from sheet \"Chart Data Export\" in datafiles/example_filters/Example filters - Delta.xlsx\n",
      "reading exported chart data from sheet \"Chart Data Export\" in datafiles/example_filters/Example filters - Rothschild.xlsx\n",
      "reading exported chart data from sheet \"Chart Data Export\" in datafiles/example_filters/Example filters - British Museum.xlsx\n",
      "reading exported chart data from sheet \"Chart Data Export\" in datafiles/example_filters/Example filters - Essity.xlsx\n",
      "reading exported chart data from sheet \"Chart Data Export\" in datafiles/example_filters/Example filters - Nokia.xlsx\n",
      "reading exported chart data from sheet \"Chart Data Export\" in datafiles/example_filters/Example filters - Charter.xlsx\n",
      "reading exported chart data from sheet \"Chart Data Export\" in datafiles/example_filters/Example filters - ADAS.xlsx\n",
      "reading exported chart data from sheet \"Chart Data Export\" in datafiles/example_filters/Example filters - Peel Hunt.xlsx\n",
      "reading exported chart data from sheet \"Chart Data Export\" in datafiles/example_filters/Example filters - SUV.xlsx\n",
      "[{'Dimension 1': 'queries', 'Dimension 2': 'categories', 'Aggregate': 'volume', 'Start Date': 'Sat Jul 01 04:00:00 UTC 2017', 'End Date': 'Wed Aug 01 04:00:00 UTC 2018', 'Project Id': '1998245961', 'Account Type': '[individual]', 'Query Id': '[1999491226]', 'Search': 'NOT RT', 'Xdomain': 'www.youtube.com', 'Xpage Type': '[facebook]', 'Xtag': 'Corporate Twitter accounts'}, {'Dimension 1': 'queryGroups', 'Dimension 2': 'queries', 'Aggregate': 'volume', 'Start Date': 'Thu Aug 01 04:00:00 UTC 2013', 'End Date': 'Wed Aug 01 04:00:00 UTC 2018', 'Project Id': '1998246280', 'Query Id': '[1999489407, 1999489385, 1999489416, 1999489400, 1999489419, 1999489387, 1999489412, 1999489382, 1999489408, 1999489395]', 'Xdomain': 'www.youtube.com', 'Xpage Type': '[facebook]', 'Xtag': 'Irrelevant'}, {'Dimension 1': 'weeks', 'Dimension 2': 'tags', 'Aggregate': 'twitterRetweets', 'Start Date': 'Sun Jan 01 00:00:00 UTC 2017', 'End Date': 'Mon Jan 01 00:00:00 UTC 2018', 'Project Id': '1998229525', 'Query Id': '[1999205209, 1999205212, 1999205215, 1999205202]', 'Xdomain': 'www.youtube.com', 'Xpage Type': '[facebook]', 'Xtag': 'Shop exclude'}, {'Dimension 1': 'days', 'Dimension 2': 'queries', 'Aggregate': 'volume', 'Start Date': 'Sun Dec 31 23:00:00 UTC 2017', 'End Date': 'Fri Aug 31 22:00:00 UTC 2018', 'Project Id': '1998248700', 'Query Id': '[1999545653]', 'Xdomain': 'www.youtube.com', 'Xpage Type': '[facebook]', 'Xtag': 'Exclusions,Irrelevant to Essity,ToiletPaper- Not Public'}, {'Dimension 1': 'weeks', 'Dimension 2': 'queries', 'Aggregate': 'volume', 'Start Date': 'Mon Jan 01 00:00:00 UTC 2018', 'End Date': 'Mon Apr 30 23:00:00 UTC 2018', 'Project Id': '1998248269', 'Query Id': '[1999536803]', 'Search': 'nokia', 'Xdomain': 'www.youtube.com', 'Xpage Type': '[facebook]'}, {'Dimension 1': 'queries', 'Dimension 2': 'parentCategories', 'Aggregate': 'volume', 'Start Date': 'Fri Jan 01 05:00:00 UTC 2016', 'End Date': 'Sat Dec 16 05:00:00 UTC 2017', 'Project Id': '1998228247', 'Parent Category': 'AA - Relevent', 'Query Id': '[1999186472, 1999186522, 1999186477]', 'Xdomain': 'www.youtube.com', 'Xpage Type': '[facebook]', 'Xtag': 'Brand query exclusions'}, {'Dimension 1': 'categories', 'Dimension 2': 'sentiment', 'Aggregate': 'volume', 'Start Date': 'Mon Jan 01 05:00:00 UTC 2018', 'End Date': 'Sun Apr 01 04:00:00 UTC 2018', 'Project Id': '1998216779', 'Query Id': '[1998992822, 1998992692, 1998992773, 1998991973, 1998992755, 1998992547, 1998990352, 1998992913, 1998992813, 1998992874, 1998992778, 1998992763]', 'Tag': 'Consumer posts', 'Xauthor Group': 'Sales Authors', 'Xdomain': 'www.youtube.com', 'Xpage Type': '[facebook]', 'Xsite Group': 'Sales Sites,YouTube', 'Xtag': 'ADAs/Law Enforcement,BW RS Exclude,Sales,Toyota Authors'}, {'Dimension 1': 'queries', 'Dimension 2': 'queries', 'Aggregate': 'volume', 'Start Date': 'Wed Feb 01 00:00:00 UTC 2017', 'End Date': 'Thu Feb 01 00:00:00 UTC 2018', 'Project Id': '1998231967', 'Query Id': '[1999244266, 1999243736, 1999244284, 1999244291, 1999244295]', 'Xauthor Group': 'Authors Exclusions', 'Xdomain': 'www.youtube.com', 'Xpage Type': '[facebook]', 'Xtag': '[BW] PH Exclusions'}, {'Dimension 1': 'queries', 'Dimension 2': 'queries', 'Aggregate': 'volume', 'Start Date': 'Sun Jan 01 05:00:00 UTC 2012', 'End Date': 'Mon Jan 01 05:00:00 UTC 2018', 'Project Id': '1998230778', 'Language': '[en]', 'Location': '[us]', 'Query Id': '[1999223818, 1999223297]', 'Xauthor Group': 'Exclusions: Author list', 'Xdomain': 'www.youtube.com', 'Xpage Type': '[facebook]', 'Xtag': 'Exclusions'}]\n"
     ]
    }
   ],
   "source": [
    "# test the function on the example files peter provided:\n",
    "import os\n",
    "example_file_dir = 'datafiles/example_filters/'\n",
    "file_list = os.listdir(example_file_dir)\n",
    "path_list = [os.path.join(example_file_dir,filename) for filename in file_list]\n",
    "\n",
    "all_the_chart_dataz = []\n",
    "for filepath in path_list:\n",
    "    chart_data = read_chart_export(filepath)\n",
    "    if chart_data != {}:\n",
    "        all_the_chart_dataz.append(chart_data)\n",
    "# a list of the information from all the example files\n",
    "print(all_the_chart_dataz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
