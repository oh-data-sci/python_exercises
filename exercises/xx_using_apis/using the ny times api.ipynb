{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using the ny times web api\n",
    "\n",
    "[earlier](files/exercises/06_web_scraping/) we saw how to read the html of a newspaper web page (theargus.co.uk) to collect the articles posted in a given day. we saw how that required to glean the structure from investigating the html first, then reverse engineer that structure to find the data we were interested in. \n",
    "\n",
    "we don't always need to do such backwards engineering. some newspapers happily share with us their articles via an application programming interface (api), under certain restrictions: \n",
    "- first, they want you to sign up and get your own api key\n",
    "- they monitor and limit your usage volume so that you can't hammer their servers with wild abandon\n",
    "- they restrict what kind of queries you can make.\n",
    "\n",
    "so how do we take advantage of their generosity? here is an example using the new york times api.\n",
    "\n",
    "- first, sign up for an api key by going [here and registering](http://developer.nytimes.com)\n",
    "- second, download and install the `nytimesarticle` module\n",
    "- then, go over the examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from nytimesarticle import articleAPI\n",
    "api = articleAPI('*Your Key Here*')\n",
    "articles = api.search(q = 'python', \n",
    "     fq = {'headline':'python', 'source':['Reuters','AP', 'The New York Times']}, \n",
    "     begin_date = 20111231)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key\n",
    "root_url = 'http://api.nytimes.com/svc/archive/v1/'\n",
    "addiction = '{year}/{month}.json?api-key={api-key}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing1980...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dc25ca57e830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1980\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2014\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processing'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mAmnesty_year\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Amnesty International'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mAmnesty_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAmnesty_all\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mAmnesty_year\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAmnesty_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-dc25ca57e830>\u001b[0m in \u001b[0;36mget_articles\u001b[0;34m(date, query)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mall_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#NYT limits pager to first 100 pages. But rarely will you find over 100 pages of results anyway.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         articles = api.search(q = query,\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mfq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Reuters'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'AP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'The New York Times'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mbegin_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'0101'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'api' is not defined"
     ]
    }
   ],
   "source": [
    "def parse_articles(articles):\n",
    "    \"\"\"\n",
    "    This function takes in a response to the NYT api and parses\n",
    "    the articles into a list of dictionaries\n",
    "    \"\"\"\n",
    "    news = []\n",
    "    for i in articles['response']['docs']:\n",
    "        dic = {}\n",
    "        dic['id'] = i['_id']\n",
    "        if i['abstract'] is not None:\n",
    "            dic['abstract'] = i['abstract'].encode(\"utf8\")\n",
    "        dic['headline'] = i['headline']['main'].encode(\"utf8\")\n",
    "        dic['desk'] = i['news_desk']\n",
    "        dic['date'] = i['pub_date'][0:10] # cutting time of day.\n",
    "        dic['section'] = i['section_name']\n",
    "        if i['snippet'] is not None:\n",
    "            dic['snippet'] = i['snippet'].encode(\"utf8\")\n",
    "        dic['source'] = i['source']\n",
    "        dic['type'] = i['type_of_material']\n",
    "        dic['url'] = i['web_url']\n",
    "        dic['word_count'] = i['word_count']\n",
    "        # locations\n",
    "        locations = []\n",
    "        for x in range(0,len(i['keywords'])):\n",
    "            if 'glocations' in i['keywords'][x]['name']:\n",
    "                locations.append(i['keywords'][x]['value'])\n",
    "        dic['locations'] = locations\n",
    "        # subject\n",
    "        subjects = []\n",
    "        for x in range(0,len(i['keywords'])):\n",
    "            if 'subject' in i['keywords'][x]['name']:\n",
    "                subjects.append(i['keywords'][x]['value'])\n",
    "        dic['subjects'] = subjects\n",
    "        news.append(dic)\n",
    "    return(news)\n",
    "def get_articles(date,query):\n",
    "    \"\"\"\n",
    "    This function accepts a year in string format (e.g.'1980')\n",
    "    and a query (e.g.'Amnesty International') and it will \n",
    "    return a list of parsed articles (in dictionaries)\n",
    "    for that year.\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "    for i in range(0,100): #NYT limits pager to first 100 pages. But rarely will you find over 100 pages of results anyway.\n",
    "        articles = api.search(q = query,\n",
    "            fq = {'source':['Reuters','AP', 'The New York Times']},\n",
    "            begin_date = date + '0101',\n",
    "            end_date = date + '1231',\n",
    "            sort='oldest',\n",
    "            page = str(i))\n",
    "        articles = parse_articles(articles)\n",
    "        all_articles = all_articles + articles\n",
    "    return(all_articles)\n",
    "\n",
    "Amnesty_all = []\n",
    "for i in range(1980,2014):\n",
    "    print('Processing' + str(i) + '...')\n",
    "    Amnesty_year = get_articles(str(i),'Amnesty International')\n",
    "    Amnesty_all = Amnesty_all + Amnesty_year\n",
    "keys = Amnesty_all[0].keys()\n",
    "with open('amnesty-mentions.csv', 'wb') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(Amnesty_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
